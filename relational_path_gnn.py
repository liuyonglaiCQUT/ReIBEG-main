import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.nn.functional import edge_softmax


class RelationalPathGNN(nn.Module):
    def __init__(self, g, ent2id, num_rel, parameter):
        super(RelationalPathGNN, self).__init__()
        self.ent2id_dict = ent2id
        self.device = parameter['device']
        self.hop = parameter['hop']
        self.es = parameter['embed_dim']
        self.g_batch = parameter['g_batch']
        self.g = g
        self.g.to(self.device)
        self.sampler = dgl.dataloading.MultiLayerFullNeighborSampler(parameter['hop'], prefetch_node_feats=['feat'],    # æ„é€ å¤šå±‚å…¨é‚»å±…é‡‡æ ·å™¨ï¼Œæ”¯æŒå¤šè·³è·¯å¾„å»ºæ¨¡ï¼ˆhopå±‚ï¼‰,é¢„å–èŠ‚ç‚¹ç‰¹å¾ featã€è¾¹ç‰¹å¾ feat å’Œ eidï¼ˆå…³ç³»IDï¼‰
                                                                     prefetch_edge_feats=['feat', 'eid'])
        self.gcn = RPGNN(self.es, self.es * 2, self.es, self.hop, num_rel)      # åˆå§‹åŒ– GCN ç¼–ç å™¨ RPGNNï¼Œè¾“å…¥ç»´åº¦ esï¼Œä¸­é—´ç»´åº¦ 2*esï¼Œè¾“å‡ºç»´åº¦ es
        self.num_rel = num_rel

    def ent2id(self, triples):  # å°†ä¸‰å…ƒç»„å®ä½“æ˜ å°„ä¸º ID
        idx = [[[self.ent2id_dict[t[0]], self.ent2id_dict[t[2]]] for t in batch] for batch in triples]  # è¾“å…¥æ˜¯å½¢å¦‚ [ [("h1", "r1", "t1"), ...], ... ] çš„ batch ä¸‰å…ƒç»„
        idx = torch.LongTensor(idx).to(self.device)
        return idx  # B * few * 2   # è¿”å›æ¯å¯¹ä¸‰å…ƒç»„çš„ (head_id, tail_id)ï¼Œå½¢çŠ¶ä¸º [B, few, 2]

    def forward(self, triples):     # å¯¹è¾“å…¥çš„å®ä½“å¯¹è¿›è¡Œ GCN ç¼–ç 
        '''
        inputs:
            task: Batch triplets, B * few
        outputs:
            emb: B * few * es
        '''

        idx = self.ent2id(triples)
        batch_size, few_shot = idx.shape[0], idx.shape[1]
        idx = idx.view(-1)
        dataloader = dgl.dataloading.DataLoader(    # ä¸ºè¿™äº›å®ä½“å¯¹åˆ›å»º DGL çš„å­å›¾ batch loaderï¼Œç”¨äºå¤šè·³ GCN ç¼–ç 
            self.g, idx, self.sampler,
            batch_size=self.g_batch,
            shuffle=False,
            drop_last=False,
            device=self.device,
            use_uva=True)
        out_emb = []
        for input_nodes, output_nodes, blocks in dataloader:    # éå†æ¯ä¸€æ‰¹é‡‡æ ·åˆ°çš„å›¾å—ï¼Œè°ƒç”¨ RPGNN ç¼–ç å™¨è¿›è¡Œå›¾å·ç§¯
            input_features = blocks[0].srcdata['feat']
            out_features = self.gcn(blocks, input_features)
            out_emb.append(out_features)
        out_emb = torch.cat(out_emb, dim=0)
        out_emb = out_emb.view(batch_size, few_shot, 2, -1)
        return out_emb  # è¾“å‡ºå½¢çŠ¶ [B, few, 2, es]ï¼Œè¡¨ç¤ºæ¯å¯¹å®ä½“è·¯å¾„å¯¹çš„ head / tail ç¼–ç 

# ç”¨äºè·¯å¾„å®ä½“å¯¹çš„å¤šå±‚å›¾å·ç§¯ç¼–ç å™¨
class RPGNN(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, hop, n_rel):
        super().__init__()
        emb_dim = in_features
        self.conv_in = RPLayer(emb_dim, in_features, hidden_features, n_rel)
        self.conv_out = RPLayer(emb_dim, hidden_features, out_features, n_rel)
        self.hop = hop
        if hop > 2:
            self.conv_hidden = nn.ModuleList(
                [RPLayer(emb_dim, hidden_features, hidden_features, n_rel) for _ in range(hop - 2)])

    def forward(self, blocks, x):   # é€šè¿‡å¤šå±‚å…³ç³»æ„ŸçŸ¥å›¾å·ç§¯æå–è·¯å¾„ä¿¡æ¯,æ¯å±‚ç”¨ä¸åŒçš„å­å›¾ block è¡¨ç¤ºä¸åŒçš„ hop
        x = F.relu(self.conv_in(blocks[0], x))
        if self.hop > 2:
            for i, conv in enumerate(self.conv_hidden):
                x = F.relu(conv(blocks[i + 1], x))
        x = F.relu(self.conv_out(blocks[-1], x))
        return x

# åŸºäºå…³ç³»æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’å±‚
class RPLayer(nn.Module):
    def __init__(self, emb_dim, in_feat, out_feat, num_rels):
        super().__init__()
        self.num_rels = num_rels
        self.linear_r = dgl.nn.pytorch.TypedLinear(in_feat + emb_dim * 2, out_feat, num_rels)
        self.attn_fc = nn.Linear(emb_dim + out_feat, 1, bias=False)
        self.h_bias = nn.Parameter(torch.Tensor(out_feat))
        self.loop_weight = nn.Parameter(torch.Tensor(emb_dim, out_feat))
        nn.init.xavier_uniform_(self.loop_weight, gain=nn.init.calculate_gain('relu'))

    def edge_agg(self, edges):
        """Relation Message Passing
        edges æ˜¯ DGL è‡ªåŠ¨ä¼ å…¥çš„è¾¹æ‰¹å¤„ç†å¯¹è±¡ï¼ŒåŒ…å«å½“å‰å­å›¾ä¸­æ‰€æœ‰çš„è¾¹
        æ¯æ¡è¾¹åŒ…å«ï¼š

        edges.src['h']ï¼šè¾¹çš„æºèŠ‚ç‚¹çš„è¾“å…¥ç‰¹å¾ï¼ˆmessageæ¥æºï¼‰

        edges.dst['feat']ï¼šè¾¹çš„ç›®æ ‡èŠ‚ç‚¹çš„åŸå§‹åµŒå…¥ï¼ˆç”¨äºæ³¨æ„åŠ›ï¼‰

        edges.data['feat']ï¼šè¾¹æœ¬èº«çš„ç‰¹å¾ï¼ˆå¯èƒ½è¡¨ç¤ºè·¯å¾„ä¸Šçš„å…³ç³»ï¼‰

        edges.data['eid']ï¼šè¾¹çš„å…³ç³»ç±»å‹ IDï¼Œç”¨äºå¤šå…³ç³»å»ºæ¨¡
        """
        x = torch.cat([edges.src['h'], edges.data['feat'], edges.dst['feat']], dim=1)   # è¿™ä¸€æ“ä½œå°†æºèŠ‚ç‚¹è¡¨ç¤º + è¾¹çš„ç‰¹å¾ + ç›®æ ‡èŠ‚ç‚¹åŸå§‹åµŒå…¥æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ç‰¹å¾å‘é‡ xï¼Œç”¨äºæ„é€ æ›´ä¸°å¯Œçš„æ¶ˆæ¯
        m = self.linear_r(x, edges.data['eid'])                     # linear_r æ˜¯ DGL çš„ TypedLinearï¼Œæ ¹æ®è¾¹çš„ eidï¼ˆå…³ç³»IDï¼‰é€‰æ‹©ä¸åŒçš„çº¿æ€§å˜æ¢çŸ©é˜µã€‚
        # attn_fc çš„è¾“å‡ºæ˜¯æ ‡é‡åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰ï¼Œä»£è¡¨è¯¥è¾¹çš„é‡è¦æ€§ ğ›¼
        attn = F.leaky_relu(self.attn_fc(torch.cat([edges.dst['feat'], m], dim=1)))     # æŠŠç›®æ ‡èŠ‚ç‚¹ç‰¹å¾ edges.dst['feat'] å’Œæ¶ˆæ¯ m æ‹¼æ¥èµ·æ¥ï¼Œé€å…¥ä¸€ä¸ªå‰é¦ˆç½‘ç»œ attn_fcï¼Œè¾“å‡ºä¸€ä¸ªæ³¨æ„åŠ›å¾—åˆ†ã€‚
        return {'h': m, 'z': attn}

    def forward(self, g, feat):
        with g.local_scope():
            # Norm
            degs = g.out_degrees().float().clamp(min=1)
            norm = torch.pow(degs, -0.5)
            shp = norm.shape + (1,) * (feat.dim() - 1)
            norm = torch.reshape(norm, shp)
            feat = feat * norm
            g.srcdata['h'] = feat
            g.apply_edges(self.edge_agg)
            e = g.edata.pop('z')
            a = edge_softmax(g, e)
            g.edata['h'] = a * g.edata['h']
            g.update_all(dgl.function.copy_e('h', 'm'), dgl.function.sum('m', 'h'))
            h = g.dstdata['h']
            h = h + g.dstdata['feat'] @ self.loop_weight
            # Norm 
            degs = g.in_degrees().float().clamp(min=1)
            norm = torch.pow(degs, -0.5)
            shp = norm.shape + (1,) * (h.dim() - 1)
            norm = torch.reshape(norm, shp)
            rst = h * norm
            h = rst + self.h_bias

            return h